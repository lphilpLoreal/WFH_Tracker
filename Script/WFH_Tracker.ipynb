{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrows heavily from Maksym Kozlenko \n",
    "\"Loading Location History Places From Google Timeline Into Pandas and CSV\".\n",
    "Link here: https://betterprogramming.pub/loading-location-history-places-from-google-timeline-into-pandas-and-csv-c26cb0ac5e89 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import json\n",
    "from timezonefinder import TimezoneFinder\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from pytz import UTC\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location data zip file:\n",
    "loc_hist = \"***PATH TO LOCATION ZIP FILE***\"\n",
    "\n",
    "#Leave data file:\n",
    "leave_extract = pd.read_csv(\"***PATH TO LEAVE CSV***\")\n",
    "\n",
    "#Output Location:\n",
    "wfh_dates_output = \"***DESIRED OUTPUT PATH***\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store places into array\n",
    "place_visits = []\n",
    "\n",
    "with ZipFile(loc_hist) as myzip:\n",
    "    for file in myzip.filelist[:]:\n",
    "        filename = file.filename\n",
    "        \n",
    "        if \"Semantic Location History\" in filename:\n",
    "            #Process all files in Semantic Location History directory\n",
    "            history_json = json.load(myzip.open(filename))\n",
    "            \n",
    "            for timeline_object in history_json[\"timelineObjects\"]:\n",
    "                \n",
    "                if \"placeVisit\" in timeline_object:\n",
    "                    place_visit_json = timeline_object[\"placeVisit\"]\n",
    "                    \n",
    "                    # Skip places without geographical coordinates\n",
    "                    if not \"location\" in place_visit_json or not \"latitudeE7\" in place_visit_json[\"location\"]:\n",
    "                        continue\n",
    "                    \n",
    "                    place_visit = {\n",
    "                        \"placeId\": place_visit_json[\"location\"][\"placeId\"],\n",
    "                        \"locationConfidence\": place_visit_json[\"location\"][\"locationConfidence\"],\n",
    "                        \"startTimestamp\": place_visit_json[\"duration\"][\"startTimestamp\"],\n",
    "                        \"endTimestamp\": place_visit_json[\"duration\"][\"endTimestamp\"],\n",
    "                        \"placeVisitImportance\": place_visit_json[\"placeVisitImportance\"],\n",
    "                        \"placeVisitType\": place_visit_json[\"placeVisitType\"],\n",
    "                        \"latitudeE7\": place_visit_json[\"location\"][\"latitudeE7\"],\n",
    "                        \"longitudeE7\": place_visit_json[\"location\"][\"longitudeE7\"],\n",
    "                    }\n",
    "                    \n",
    "                    for optional_field in [\"centerLatE7\", \"centerLngE7\"]:\n",
    "                        if optional_field in place_visit_json:\n",
    "                            place_visit[optional_field] = place_visit_json[optional_field]\n",
    "                        else:\n",
    "                            place_visit[optional_field] = None\n",
    "                    \n",
    "                    for optional_field in [\"name\", \"address\"]:\n",
    "                        if optional_field in place_visit_json[\"location\"]:\n",
    "                            place_visit[optional_field] = place_visit_json[\"location\"][optional_field]\n",
    "                        else:\n",
    "                            place_visit[optional_field] = None\n",
    "                        \n",
    "                    \n",
    "                    place_visits.append(place_visit)\n",
    "\n",
    "place_visits_df = pd.DataFrame(place_visits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime type\n",
    "place_visits_df[\"startTimestamp\"] = pd.to_datetime(place_visits_df[\"startTimestamp\"])\n",
    "place_visits_df[\"endTimestamp\"] = pd.to_datetime(place_visits_df[\"endTimestamp\"])\n",
    "\n",
    "# get geo coordinates as float value\n",
    "place_visits_df[\"latitude\"] = place_visits_df.latitudeE7/1E7\n",
    "place_visits_df[\"longitude\"] = place_visits_df.longitudeE7/1E7\n",
    "place_visits_df[\"centerLat\"] = place_visits_df.centerLatE7/1E7\n",
    "place_visits_df[\"centerLng\"] = place_visits_df.centerLngE7/1E7\n",
    "\n",
    "# add timezone based on geo coordinates\n",
    "tf = TimezoneFinder()\n",
    "place_visits_df[\"timezone\"] = place_visits_df.apply(lambda row: tf.timezone_at(lng=row.longitude, lat=row.latitude), axis=1)\n",
    "\n",
    "# convert UTC time to local timezone\n",
    "place_visits_df['startTimestamp_local'] = place_visits_df.apply(lambda row: row.startTimestamp.tz_convert(row.timezone), axis=1)\n",
    "place_visits_df['endTimestamp_local'] =place_visits_df.apply(lambda row: row.endTimestamp.tz_convert(row.timezone), axis=1)\n",
    "\n",
    "# remove TZ info from datetime\n",
    "place_visits_df['startTimestamp_local'] = pd.to_datetime(place_visits_df['startTimestamp_local'].apply(lambda x: x.replace(tzinfo=None)))\n",
    "place_visits_df['endTimestamp_local'] = pd.to_datetime(place_visits_df['endTimestamp_local'].apply(lambda x: x.replace(tzinfo=None)))\n",
    "\n",
    "# add datetime parts as a separate column to data frame\n",
    "for datetime_type in [(\"year\", lambda x: x.year), (\"month\", lambda x: x.month), (\"day\", lambda x: x.day), (\"hour\", lambda x: x.hour), (\"minute\", lambda x: x.minute), (\"weekday\", lambda x: x.weekday)]:\n",
    "   for tztype in [\"\", \"_local\"]:\n",
    "       place_visits_df[f\"{datetime_type[0]}{tztype}\"] = datetime_type[1](place_visits_df[f\"startTimestamp{tztype}\"].dt)\n",
    "     \n",
    "place_visits_df.drop(columns=[\"latitudeE7\", \"longitudeE7\", \"centerLatE7\", \"centerLngE7\"], inplace=True)\n",
    "\n",
    "place_visits_df[\"duration\"] = place_visits_df.endTimestamp - place_visits_df.startTimestamp\n",
    "place_visits_df[\"duration_minutes\"] = place_visits_df.duration.dt.total_seconds()/60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Days working at office and home in financial year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Year Dates\n",
    "start_date = pd.to_datetime('2022-07-01').tz_localize(UTC)\n",
    "end_date = pd.to_datetime('2023-06-30').tz_localize(UTC)\n",
    "\n",
    "dates_of_interest = place_visits_df[\n",
    "    (place_visits_df.startTimestamp >= start_date) & \n",
    "    (place_visits_df.startTimestamp <= end_date)\n",
    "]\n",
    "dates_of_interest['startTimestamp_local'] = dates_of_interest['startTimestamp_local'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# dates_of_interest['date'] = dates_of_interest['year_local'].astype(str) + '-' + dates_of_interest['month_local'].astype(str) + '-' + dates_of_interest['day_local'].astype(str)\n",
    "dates_of_interest.drop(\n",
    "    columns=[\n",
    "        'startTimestamp',\n",
    "        'endTimestamp',\n",
    "        'year',\n",
    "        'year_local',\n",
    "        'month',\n",
    "        'month_local',\n",
    "        'day',\n",
    "        'day_local',\n",
    "        'hour',\n",
    "        'minute',\n",
    "        'weekday',\n",
    "        'duration',\n",
    "        'duration_minutes',\n",
    "        'weekday'\n",
    "    ])\n",
    "\n",
    "dates_of_interest = dates_of_interest[~dates_of_interest['weekday_local'].isin([5, 6])]\n",
    "\n",
    "office_dates = dates_of_interest.loc[dates_of_interest['name'] == 'L’Oréal Australia', 'startTimestamp_local'].unique()\n",
    "office_dates = pd.DataFrame({'date': office_dates})\n",
    "\n",
    "wfh_dates = dates_of_interest[~dates_of_interest['startTimestamp_local'].isin(office_dates['date'])].copy()\n",
    "wfh_dates = wfh_dates.drop_duplicates(subset='startTimestamp_local')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Leave Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime format\n",
    "leave_extract['Start Date'] = pd.to_datetime(leave_extract['Start Date'], format='%d/%m/%Y', errors='coerce')\n",
    "leave_extract['End Date'] = pd.to_datetime(leave_extract['End Date'], format='%d/%m/%Y', errors='coerce')\n",
    "\n",
    "# Drop rows with missing or invalid dates\n",
    "leave_extract = leave_extract.dropna(subset=['Start Date', 'End Date'])\n",
    "\n",
    "# Create a list of all dates between 'Start Date' and 'End Date' for each row\n",
    "date_ranges = [pd.date_range(start, end) for start, end in zip(leave_extract['Start Date'], leave_extract['End Date'])]\n",
    "\n",
    "# Create a new dataframe with individual days as separate rows\n",
    "expanded_dates = pd.DataFrame({'Date': [date for date_range in date_ranges for date in date_range]})\n",
    "\n",
    "# Merge the expanded dates with the original leave_extract dataframe\n",
    "leave_df = pd.merge(expanded_dates, leave_extract, how='left', left_on='Date', right_on='Start Date')\n",
    "\n",
    "# Drop unnecessary columns and reset the index\n",
    "leave_df = leave_df.drop(columns=['Start Date', 'End Date', 'Days Taken']).reset_index(drop=True)\n",
    "leave_df['Date'] = leave_df['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#Remove Leave from Days Worked From Home\n",
    "wfh_dates = wfh_dates[~wfh_dates['startTimestamp_local'].isin(leave_df['Date'])].copy()\n",
    "\n",
    "wfh_dates = wfh_dates.drop(columns=[\n",
    "    'placeId',\n",
    "    'address',\n",
    "    'locationConfidence',\n",
    "    'startTimestamp',\n",
    "    'endTimestamp',\n",
    "    'placeVisitImportance',\n",
    "    'placeVisitType',\n",
    "    'name',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'centerLat',\n",
    "    'centerLng',\n",
    "    'timezone',\n",
    "    'endTimestamp_local',\n",
    "    'year',\n",
    "    'year_local',\n",
    "    'month',\n",
    "    'month_local',\n",
    "    'day',\n",
    "    'day_local',\n",
    "    'hour',\n",
    "    'hour_local',\n",
    "    'minute',\n",
    "    'minute_local',\n",
    "    'weekday',\n",
    "    'duration',\n",
    "    'duration_minutes',\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfh_dates.to_csv(wfh_dates_output)\n",
    "wfh_dates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
